{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic.', 'Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs.', 'This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.']\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs. This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = nltk.sent_tokenize(corpus)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'that',\n",
       " 'are',\n",
       " 'organized',\n",
       " 'and',\n",
       " 'coherent',\n",
       " ',',\n",
       " 'and',\n",
       " 'are',\n",
       " 'all',\n",
       " 'related',\n",
       " 'to',\n",
       " 'a',\n",
       " 'single',\n",
       " 'topic',\n",
       " '.',\n",
       " 'Almost',\n",
       " 'every',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'writing',\n",
       " 'you',\n",
       " 'do',\n",
       " 'that',\n",
       " 'is',\n",
       " 'longer',\n",
       " 'than',\n",
       " 'a',\n",
       " 'few',\n",
       " 'sentences',\n",
       " 'should',\n",
       " 'be',\n",
       " 'organized',\n",
       " 'into',\n",
       " 'paragraphs',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'because',\n",
       " 'paragraphs',\n",
       " 'show',\n",
       " 'a',\n",
       " 'reader',\n",
       " 'where',\n",
       " 'the',\n",
       " 'subdivisions',\n",
       " 'of',\n",
       " 'an',\n",
       " 'essay',\n",
       " 'begin',\n",
       " 'and',\n",
       " 'end',\n",
       " ',',\n",
       " 'and',\n",
       " 'thus',\n",
       " 'help',\n",
       " 'the',\n",
       " 'reader',\n",
       " 'see',\n",
       " 'the',\n",
       " 'organization',\n",
       " 'of',\n",
       " 'the',\n",
       " 'essay',\n",
       " 'and',\n",
       " 'grasp',\n",
       " 'its',\n",
       " 'main',\n",
       " 'points',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import nltk\n",
    "\n",
    "# Download all NLTK data\n",
    "nltk.download('all')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'paragraph', 'is', 'a', 'series', 'of', 'sentences', 'that', 'are', 'organized', 'and', 'coherent', ',', 'and', 'are', 'all', 'related', 'to', 'a', 'single', 'topic', '.', 'Almost', 'every', 'piece', 'of', 'writing', 'you', 'do', 'that', 'is', 'longer', 'than', 'a', 'few', 'sentences', 'should', 'be', 'organized', 'into', 'paragraphs', '.', 'This', 'is', 'because', 'paragraphs', 'show', 'a', 'reader', 'where', 'the', 'subdivisions', 'of', 'an', 'essay', 'begin', 'and', 'end', ',', 'and', 'thus', 'help', 'the', 'reader', 'see', 'the', 'organization', 'of', 'the', 'essay', 'and', 'grasp', 'its', 'main', 'points', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\debas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Ensure the necessary resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the corpus\n",
    "corpus = \"\"\"\n",
    "A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs. This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Perform word tokenization\n",
    "words = word_tokenize(corpus)\n",
    "\n",
    "# Print the tokenized words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'that',\n",
       " 'are',\n",
       " 'organized',\n",
       " 'and',\n",
       " 'coherent',\n",
       " ',',\n",
       " 'and',\n",
       " 'are',\n",
       " 'all',\n",
       " 'related',\n",
       " 'to',\n",
       " 'a',\n",
       " 'single',\n",
       " 'topic',\n",
       " '.',\n",
       " 'Almost',\n",
       " 'every',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'writing',\n",
       " 'you',\n",
       " 'do',\n",
       " 'that',\n",
       " 'is',\n",
       " 'longer',\n",
       " 'than',\n",
       " 'a',\n",
       " 'few',\n",
       " 'sentences',\n",
       " 'should',\n",
       " 'be',\n",
       " 'organized',\n",
       " 'into',\n",
       " 'paragraphs',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'because',\n",
       " 'paragraphs',\n",
       " 'show',\n",
       " 'a',\n",
       " 'reader',\n",
       " 'where',\n",
       " 'the',\n",
       " 'subdivisions',\n",
       " 'of',\n",
       " 'an',\n",
       " 'essay',\n",
       " 'begin',\n",
       " 'and',\n",
       " 'end',\n",
       " ',',\n",
       " 'and',\n",
       " 'thus',\n",
       " 'help',\n",
       " 'the',\n",
       " 'reader',\n",
       " 'see',\n",
       " 'the',\n",
       " 'organization',\n",
       " 'of',\n",
       " 'the',\n",
       " 'essay',\n",
       " 'and',\n",
       " 'grasp',\n",
       " 'its',\n",
       " 'main',\n",
       " 'points',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'that',\n",
       " 'are',\n",
       " 'organized',\n",
       " 'and',\n",
       " 'coherent',\n",
       " ',',\n",
       " 'and',\n",
       " 'are',\n",
       " 'all',\n",
       " 'related',\n",
       " 'to',\n",
       " 'a',\n",
       " 'single',\n",
       " 'topic.',\n",
       " 'Almost',\n",
       " 'every',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'writing',\n",
       " 'you',\n",
       " 'do',\n",
       " 'that',\n",
       " 'is',\n",
       " 'longer',\n",
       " 'than',\n",
       " 'a',\n",
       " 'few',\n",
       " 'sentences',\n",
       " 'should',\n",
       " 'be',\n",
       " 'organized',\n",
       " 'into',\n",
       " 'paragraphs.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'because',\n",
       " 'paragraphs',\n",
       " 'show',\n",
       " 'a',\n",
       " 'reader',\n",
       " 'where',\n",
       " 'the',\n",
       " 'subdivisions',\n",
       " 'of',\n",
       " 'an',\n",
       " 'essay',\n",
       " 'begin',\n",
       " 'and',\n",
       " 'end',\n",
       " ',',\n",
       " 'and',\n",
       " 'thus',\n",
       " 'help',\n",
       " 'the',\n",
       " 'reader',\n",
       " 'see',\n",
       " 'the',\n",
       " 'organization',\n",
       " 'of',\n",
       " 'the',\n",
       " 'essay',\n",
       " 'and',\n",
       " 'grasp',\n",
       " 'its',\n",
       " 'main',\n",
       " 'points',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##stemming \n",
    "''' \n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words know as a lemma. \n",
    "stemmng is important in natural language understanding and natural language processing\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##classification promblem\n",
    "##comments of product is a positive review or negative review\n",
    "review----------->eating, eat, eaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['eating','eats','eaten','writing','writes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------->eat\n",
      "eats------->eat\n",
      "eaten------->eaten\n",
      "writing------->write\n",
      "writes------->write\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------->\"+stemming.stem(word))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegexpStemmer class \n",
    "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. it basically takes a single regular expression  and removes any prefix or suffix that matches the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'was'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$',min=4)\n",
    "\n",
    "reg_stemmer.stem('was')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer=SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---------->eat\n",
      "eats---------->eat\n",
      "eaten---------->eaten\n",
      "writing---------->write\n",
      "writes---------->write\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'---------->'+snowballstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
